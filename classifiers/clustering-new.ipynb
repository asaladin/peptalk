{
 "metadata": {
  "name": "clustering-new"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering SVM results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to get meaningful results from our classifier, we cluster positively-predicted residues geometrically.\n",
      "For each receptor entry in PeptiDB, a set of residue clusters is formed, ranked by size.\n",
      "\n",
      "Later we evaluate the success of this method by the recall of binding energy assigned to each cluster separately, and the top-3 combined."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, import all necessary modules. Specifically, note the import of `peptalk` (last line), the module that implements clustering for a given receptor, and some statistics about its success."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, preprocessing, svm, cross_validation, metrics\n",
      "\n",
      "import pylab as pl\n",
      "import pandas as pd\n",
      "\n",
      "import numpy as np\n",
      "import os, sys\n",
      "import csv\n",
      "from pickle import dump, load\n",
      "\n",
      "from treedict import TreeDict\n",
      "\n",
      "import peptalk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next two cells prepare the SVM output for clustering. \n",
      "The result is `classifier`, an SVM classifier trained on the whole bound receptor set in PeptiDB.\n",
      "\n",
      "We load a *cached version* (\"pickled\") of the `classifier` object, if one is available, \n",
      "to avoid fitting the classifier every time we need its results.\n",
      "If one isn't the classifier is fitted and pickled for subsequent use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import config, data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(data)\n",
      "reload(config)\n",
      "reload(peptalk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_tdict = TreeDict()\n",
      "for context in ('bound', 'unbound'):\n",
      "    for version in ('new', 'new_reduced', 'old',):\n",
      "        data_tdict.update({'.'.join([version, context]) : \n",
      "        data.prepDataSet('%s.data.%s.csv' % (context, version), \n",
      "                         dataset_name='peptalk.{}.{}'.format(context, version),\n",
      "                        )\n",
      "                 })\n",
      "datasets = data_tdict.interactiveTree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prepare grid of ROC curves per PDB:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['figure.figsize'] = (10.0, 10.0)\n",
      "rcParams['font.size'] = 10.0\n",
      "rcParams['text.usetex'] = True\n",
      "rcParams['legend.fontsize'] = 'small'\n",
      "rcParams['legend.loc'] = 'upper left'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cutoff_cfgs = {cutoff: config.createConfig(feature_set=datasets.old.unbound.feature_set, \n",
      "                            train=datasets.old.unbound.csv_filename, \n",
      "                            test=datasets.old.bound.csv_filename, \n",
      "                            ddg_cutoff=cutoff) for cutoff in (0, 1)}\n",
      "\n",
      "pdbs = cutoff_cfgs[0].testing.label_data_df.index.levels[0][:14]\n",
      "\n",
      "w = int(4)\n",
      "h = int(ceil(len(pdbs) / float(w)))\n",
      "rcParams['figure.figsize'] = (3.2*w, 2.5*h)\n",
      "\n",
      "for i, pdb in enumerate(pdbs, 1):\n",
      "    ax = pl.subplot(h, w, i)\n",
      "    for cutoff, cfg in cutoff_cfgs.items():\n",
      "        pdb_clf = peptalk.Classifier(pdb, cfg)\n",
      "        fpr, tpr, thresholds = metrics.roc_curve(pdb_clf.mask_binding, pdb_clf.confidence)\n",
      "        if not np.isfinite(tpr).all():\n",
      "            tpr = np.zeros(tpr.shape) + 0.01\n",
      "        fpr_grid = np.linspace(0, 1, 100)\n",
      "        tpr_interp = interp(fpr_grid, fpr, tpr, left=0, right=1)\n",
      "        roc_auc = metrics.auc(fpr_grid, tpr_interp)\n",
      "        accuracy = metrics.accuracy_score(pdb_clf.mask_binding, pdb_clf.mask_positive)\n",
      "        ax.plot(\n",
      "                fpr_grid, tpr_interp, \n",
      "                label='%s cutoff=%0.1f\\n(AUC = %0.2f)' % (pdb, cutoff, roc_auc),\n",
      "                )\n",
      "        \n",
      "    ax.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
      "    \n",
      "    ax.set_xlim([-0.05, 1.05])\n",
      "    ax.set_ylim([-0.05, 1.05])\n",
      "    #ax.set_xlabel('False Positive Rate')\n",
      "    #ax.set_ylabel('True Positive Rate')\n",
      "    #ax.title('Comparison of classifier configurations' + '\\n'+\n",
      "    #        'Mean ROC curves, performance measured on test set')\n",
      "    ax.legend(loc='lower right')\n",
      "    #ax.set_title(pdb)\n",
      "    #ax.show()\n",
      "pl.savefig('pdb_roc.svg')\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Prepare clustering data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cluster SVM results for all receptors in PeptiDB 1, and collect statistics in a CSV table about every cluster for subsequent analysis:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(peptalk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_stats_fd = open('cluster-stats.csv', 'w')\n",
      "\n",
      "csvwr = csv.writer(cluster_stats_fd,)# delimiter='\\t')\n",
      "\n",
      "csvwr.writerow(\n",
      "                (\n",
      "                'PDBID', \n",
      "                'METHOD', \n",
      "                'RANK', \n",
      "                'SIZE', \n",
      "                'ACCURACY_SCORE',\n",
      "                'CLUSTER_DDG',\n",
      "                'RECOVERED_DDG',\n",
      "                'TOTAL_DDG',\n",
      "                'ENERGY_RECALL', \n",
      "                'ENERGY_RECALL_POSSIBLE', \n",
      "                'HOTSPOT_RECALL', \n",
      "                'HOTSPOT_PRECISION', \n",
      "                'ENERGY_F2', \n",
      "                'BINDERS_RECALL', \n",
      "                'BINDERS_PRECISION', \n",
      "                'RESIDUES',\n",
      "                )\n",
      "              )\n",
      "\n",
      "cfg = cutoff_cfgs[0]\n",
      "for pdbid in cfg.testing.label_data_df.index.levels[0]:\n",
      "    print pdbid\n",
      "    svm_result = peptalk.Classifier(pdbid, cfg)\n",
      "    ward_cl = svm_result.cluster_ward()\n",
      "    dbscan_cl = svm_result.cluster_dbscan()\n",
      "    naive5_cl = svm_result.cluster_naive(k=5)\n",
      "    naive10_cl = svm_result.cluster_naive(k=10)\n",
      "    for method, cluster_dict in (('DBSCAN', dbscan_cl), ('Ward', ward_cl), ('Naive5', naive5_cl), ('Naive10', naive10_cl),):\n",
      "        for rank, resnums in cluster_dict.items():\n",
      "            cluster_size = len(resnums)\n",
      "            cluster_ddg = svm_result.cluster_ddg(resnums)\n",
      "            accuracy = svm_result.cluster_accuracy(resnums)\n",
      "            ddg_rcl, ddg_rcl_possible = svm_result.cluster_ddg_recall(resnums)\n",
      "            hs_recl, hs_prec = svm_result.cluster_recall_precision(resnums,)# svm_result.binding_resnums)\n",
      "            #bs_recl, bs_prec = svm_result.cluster_recall_precision(resnums, svm_result.binders_resnums)\n",
      "            def fbeta_score(recall, precision, beta=1):\n",
      "                if recall==0 and precision==0: return 0\n",
      "                return (1 + beta**2) * float(precision*recall) / float(beta**2 * precision + recall)\n",
      "            f2_ddg = fbeta_score(recall=ddg_rcl_possible, precision=hs_prec, beta=1)\n",
      "            csvwr.writerow([\n",
      "                            pdbid,\n",
      "                            method,\n",
      "                            rank,\n",
      "                            cluster_size,\n",
      "                            accuracy,\n",
      "                            cluster_ddg,\n",
      "                            svm_result.recovered_ddg,\n",
      "                            svm_result.total_ddg,\n",
      "                            ddg_rcl,\n",
      "                            ddg_rcl_possible,\n",
      "                            hs_recl,\n",
      "                            hs_prec,\n",
      "                            f2_ddg,\n",
      "                            #bs_recl,\n",
      "                            #bs_prec,\n",
      "                            ' '.join(map(str, resnums)),\n",
      "                            ])\n",
      "        #break\n",
      "\n",
      "cluster_stats_fd.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read the CSV table generated above into `cluster_stats`, \n",
      "a Data Frame object - kind of a spreadsheet that can hold data and manipulate it in cool ways."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_stats = pd.read_csv('cluster-stats.csv',)\n",
      "# here's a quick peek at the top 5 rows:\n",
      "display(cluster_stats.ix[cluster_stats.PDBID=='1CZY'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create an object called `colnames` to represent column names programmatically.\n",
      "It saves the need to specify a magic string when addressing a column name in the data frame.\n",
      "\n",
      "So instead of `cluster_stats['PDBID']` you can write `cluster_stats[colnames.PDBID]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Struct:\n",
      "    def __init__(self, **entries): \n",
      "        self.__dict__.update(entries)\n",
      "\n",
      "cols = cluster_stats.columns.tolist()\n",
      "colnames = Struct(**dict(zip(cols, cols)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "st = cluster_stats.ix[\n",
      "                      (cluster_stats.METHOD != 'Ward')\n",
      "                      & (cluster_stats.RANK == 0)\n",
      "                      ]\n",
      "st = st.pivot_table(rows=[colnames.PDBID], cols=[colnames.METHOD,], values=[colnames.ENERGY_RECALL_POSSIBLE]) > .5\n",
      "#st.ENERGY_RECALL_POSSIBLE['Naive']\n",
      "pd.crosstab(rows=st.ENERGY_RECALL_POSSIBLE['Naive5'], cols=st.ENERGY_RECALL_POSSIBLE['DBSCAN'], )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cluster size distribution in DBSCAN results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['figure.figsize'] = (6,4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_clusters = cluster_stats.ix[\n",
      "                    (cluster_stats.METHOD!='Naive5') & \n",
      "                    (cluster_stats.METHOD!='Naive10') & \n",
      "                    (cluster_stats.ENERGY_RECALL_POSSIBLE > 0),\n",
      "                    (colnames.METHOD,colnames.SIZE)\n",
      "                    ]\n",
      "size_pvt = db_clusters.pivot_table(\n",
      "            rows=colnames.SIZE, \n",
      "            #cols=colnames.RANK, \n",
      "            cols=colnames.METHOD,\n",
      "            aggfunc=np.count_nonzero, \n",
      "            fill_value=0,\n",
      "        )\n",
      "size_pvt.plot()#subplots=True, sharex=True, sharey=True, yticks=range(0,61,20), )\n",
      "pl.ylabel('Frequency')\n",
      "pl.xlabel('Cluster size')\n",
      "pl.show()\n",
      "#savefig('interactive1.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "st = cluster_stats.ix[\n",
      "                (cluster_stats.RANK<3) \n",
      "                #& (cluster_stats.RECOVERED_DDG > .5 * cluster_stats.TOTAL_DDG) \n",
      "                #& (cluster_stats.METHOD=='DBSCAN')\n",
      "                ]\n",
      "display(st[:12])\n",
      "gr=st.groupby([colnames.PDBID, colnames.METHOD], as_index=False)\n",
      "pick_best = lambda clusters: clusters.sort(columns=[colnames.ENERGY_RECALL_POSSIBLE, colnames.ENERGY_RECALL]).tail(1)\n",
      "best = gr.aggregate(pick_best, )\n",
      "best[:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance comparison between DBSCAN and Ward clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define `topk_performance`, a function that gathers data about the top-k clusters of each method in a pivot table,\n",
      "outputs a PDBID-wise comparison between them, generates a bar chart of their recall distribution and summarizing statistics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.set_option('display.precision',3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['text.usetex'] = True\n",
      "rcParams['figure.figsize'] = (6.0, 4.0)\n",
      "\n",
      "def topk_performance(cluster_stats, k=1):\n",
      "    # filter the data to ranks < k, and where the SVM classified positively \n",
      "    # enough residues to comprise at least 50% of the total ddG\n",
      "    topk_clusters = cluster_stats.ix[\n",
      "                        (cluster_stats.RANK<k) \n",
      "                        & (cluster_stats.RECOVERED_DDG > .5 * cluster_stats.TOTAL_DDG) \n",
      "                        #& (cluster_stats.METHOD=='DBSCAN')\n",
      "                        ]\n",
      "    \n",
      "    # pivot the data to aggregate the top-k clusters for each method\n",
      "    # and report the sum of ddG-related parameters over the top-k clusters\n",
      "    method_pvt = topk_clusters.pivot_table(\n",
      "                rows=colnames.PDBID, \n",
      "                cols=  [\n",
      "                        colnames.METHOD,\n",
      "                        #colnames.RANK,\n",
      "                        ], \n",
      " \n",
      "                values=[\n",
      "                        colnames.SIZE,\n",
      "                        colnames.ENERGY_RECALL,\n",
      "                        colnames.ENERGY_RECALL_POSSIBLE,\n",
      "                        colnames.ENERGY_F2,\n",
      "                        #colnames.RECOVERED_DDG,\n",
      "                        #colnames.TOTAL_DDG,\n",
      "                        colnames.HOTSPOT_RECALL,\n",
      "                        colnames.BINDERS_RECALL, \n",
      "                        #colnames.HOTSPOT_PRECISION, \n",
      "                        #colnames.BINDERS_PRECISION,\n",
      "                       ], \n",
      "                        aggfunc=np.sum,\n",
      "                        #margins=True,\n",
      "              )\n",
      "    recalls=method_pvt[colnames.ENERGY_RECALL_POSSIBLE]\n",
      "    assert np.max(recalls.values) <= 1.0001, np.max(recalls.values)\n",
      "    \n",
      "    #display(method_pvt.head(10))#.ix[:,:4])\n",
      "    \n",
      "    recall_bins = linspace(0, 1.0, num=6, endpoint=True)\n",
      "    pl.hist([recalls[col] for col in recalls.columns], \n",
      "            bins=recall_bins, \n",
      "            label=recalls.columns.tolist(), \n",
      "            alpha=.8,\n",
      "            )\n",
      "    \n",
      "    pl.ylim(0, 75)\n",
      "    \n",
      "    pl.title('''Distribution of $\\Delta\\Delta G$ recall of top-%d clusters \n",
      "                for DBSCAN and Ward clustering methods''' % k)\n",
      "    pl.grid(axis='y')\n",
      "    pl.legend(loc='upper left')\n",
      "    pl.xlabel('Success rate ($ \\Delta\\Delta G $ recall)')\n",
      "    pl.xticks(recall_bins)\n",
      "    pl.ylabel('Frequency')\n",
      "    \n",
      "    pl.show()\n",
      "    \n",
      "    # overall performance of the two methods over different parameters\n",
      "    m=method_pvt.mean().unstack(1)\n",
      "    display(m)\n",
      "    display(method_pvt.describe())\n",
      "    return recalls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Performance of top cluster in each method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r1 = topk_performance(cluster_stats, k=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r1.ix[\n",
      "     (r1.DBSCAN - r1.Naive5).abs() > .75\n",
      "     ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Performance of top-3 clusters of each method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r3 = topk_performance(cluster_stats, k=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r3.ix[\n",
      "     (r3.DBSCAN - r3.Naive5).abs() > .75\n",
      "     ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parameter comparison on DBSCAN results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part I compare the success rate I measure over two definitions of the binding site. \n",
      "This leads me to lean toward the positive-ddG energy-based definition, rather than the 4A-cutoff distance-based definition."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The ddG recovered by different classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ddg_pvt = cluster_stats.ix[\n",
      "                    (cluster_stats.RANK==0) &\n",
      "                    (cluster_stats.METHOD=='DBSCAN')\n",
      "                    ].pivot_table(\n",
      "                rows=colnames.PDBID, \n",
      "                cols=  [\n",
      "                        #colnames.METHOD,\n",
      "                        #colnames.RANK,\n",
      "                        ], \n",
      " \n",
      "                values=[\n",
      "                        #colnames.ENERGY_RECALL,\n",
      "                        #colnames.ENERGY_RECALL_POSSIBLE,\n",
      "                        #colnames.ENERGY_F2,\n",
      "                        #colnames.CLUSTER_DDG,\n",
      "                        colnames.RECOVERED_DDG,\n",
      "                        colnames.TOTAL_DDG,\n",
      "                        #colnames.HOTSPOT_RECALL,\n",
      "                        #colnames.BINDERS_RECALL, \n",
      "                        #colnames.HOTSPOT_PRECISION, \n",
      "                        #colnames.BINDERS_PRECISION,\n",
      "                       ], \n",
      "                        aggfunc=np.sum,\n",
      "                        #margins=True,\n",
      "              )\n",
      "ddg_pvt['RECOVERY_RATE'] = ddg_pvt.RECOVERED_DDG / ddg_pvt.TOTAL_DDG\n",
      "display(ddg_pvt.head())\n",
      "display(ddg_pvt.describe())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Obsolete content"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "coloring clusters by rank"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualizing clustering results with ProDy `showProtein`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prody.SETTINGS['auto_show']=False\n",
      "showProtein(prot, A='grey', B='magenta', label='prot', lw=3)\n",
      "s = positives.copy()\n",
      "s.setTitle('positive')\n",
      "cluster_colors = {\n",
      "        'CL0_': 'Red',\n",
      "        'CL1_': 'DarkOrange',\n",
      "        'CL2_': 'Coral',\n",
      "        'CL3_': 'Yellow',\n",
      "        'CL4_': 'White',\n",
      "        'CL-1_': 'Green',\n",
      "        }\n",
      "   \n",
      "s.ca.setResnames(['CL%s_' % str(label) for label in cluster_labels])\n",
      "print cluster_colors\n",
      "showProtein(s, hsize=10, label='positives', **cluster_colors)\n",
      "pl.legend(loc='upper left')\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Produce a copy of `prot` with residues in `resnums` having beta numbers from `bfactors`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def modifiedBetas(prot, resnums, bfactors):\n",
      "    p = prot.copy()\n",
      "    p.setBetas([0]*len(p))\n",
      "    \n",
      "    h = p.select('chain A and resnum %s' % ' '.join(map(str, resnums))).getHierView()\n",
      "    for resnum, bfactor in zip(resnums, bfactors):\n",
      "        res = h.getResidue('A', resnum)\n",
      "        res.setBetas([bfactor]*len(res))\n",
      "    \n",
      "    return p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "scoring clusters using size and planarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measure the planarity of a set of coordinates using PCA:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def planarity(coords):\n",
      "    pca = decomposition.PCA()\n",
      "    pca.fit(coords)\n",
      "    return 1-pca.explained_variance_ratio_[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Score a set of clusters based on size and planarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_clusters(coords, cluster_labels):\n",
      "    clusters = [coords[cluster_labels==i] for i in range(N_CLUSTERS)]\n",
      "    planarities = np.array([planarity(cl_coords) for cl_coords in clusters])\n",
      "    plana = planarities #preprocessing.scale(planarities)\n",
      "    \n",
      "    print plana\n",
      "    sizes = np.array(map(len, clusters))\n",
      "    print sizes\n",
      "    \n",
      "    scores = plana * sizes\n",
      "    return scores\n",
      "    #print len(coords), planarity,\n",
      "    #return len(coords) * planarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}